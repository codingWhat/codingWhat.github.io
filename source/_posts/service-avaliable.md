---
title: 服务可用性-理论篇
date: 2024-07-28 17:52:10
tags:
- GO
- 微服务
- 服务可用性
- 可用性治理
---

# 定义

服务可用性是指`服务正常运行的时间/服务实际运行的时间`, 不过专业来说应该是`可用性=MTTF / (MTTR+MTTF) * 100%`, MTTF(Mean time to failure) 平均无故障时间，MTTR(Mean time to repair)平均故障修复时间,MTTF衡量可用性。
MTTF越长说明系统可用性越好，越稳定。MTTR则代表系统恢复速度，值越小说明系统鲁棒性越好。
![MTTF、MTTR、MTBF](/images/available_metric.png)

# 重要性
假设用户使用你的产品不是在报错、就是白屏，系统频繁故障，你想想会发生什么？我们来详细分析下这个问题。  
从用户角度来看:
- 信任感越来越低
- 体验感越来越差
- 卸载app

从公司角度来看:
- 收入损失
- 口碑一落千丈
- 影响股价
- 舆论压力

从开发来角度看:
- 加班
- 325，半年白干
- 可能工作都没了

综上来看可用性必须要足够重视，作为开发一定要竭尽所能保证系统可用性(稳定)。

# 故障的种类
![故障种类](/images/sa_fail_type.png)

如图故障主要分为以上5类，其中只有`变更类`是主动触发的，其他都是外部因素导致的，不过还是需要重视和解决。

# 如何衡量可用性?
知道了可用性要解决的问题域，接下来我们详细看下可用性指标。
![可用性公式](/images/sa_formula.png)


这里的关键是`MTTR`-平均故障修复时间，而`MTTR`又可以分为:
`MTTI`(mean time to identify), 平均故障识别时间，这个阶段会通过各个渠道(告警、舆情、用户反馈)收到故障信息  
`MTTK`(mean time to know), 平均故障认知时间，定位业务、定位对应开发、开发定位问题根因所花时间  
`MTTS`(mean time to solve), 平均故障解决时间，这部分主要包含上线修复、验证的时间

![MTTR细节](/images/sa_mttr_detail.png)

# 如何提升可用性?

基于已知的`可用性公式`，只能缩短`MTTR`，也就是说:  `减少故障数量`、`加快问题发现定位[MTTI+MTTK]`以及`加快问题解决[MTTS]`。

![提升可用性](/images/sa_pre_handle_fail_and_fail_identify_solve.png)

## 减少故障数量【事前】
![上线前 + 上线中](/images/sa_pre_online.png)
### 代码质量 
- 建立代码规范，统一标准  
- 严格的code review  
### 架构设计
- 异步解耦和削峰设计（消息队列）
- 可扩展性设计; 分层、无状态
### 风险控制
- 限流
- 降级
- 熔断
- 重试
- 隔离
- 兼容性

常规治理手段: 可查看《[服务可用性治理](https://codingwhat.github.io/2024/07/17/serive-high-available-governance/)》


### 容量评估
- 对于提前预知的流量(春节、赛事活动等)，提前做好扩容。
- 预留容量buffer
- 弹性伸缩


### 测试验证
- 单测、集成测试
- 回归测试、功能测试、兼容性测试

### 变更规范
- 发布顺序
- 可灰度，代码发布, 金丝雀发布，将故障影响降到最低；配置变更支持灰度
- 可监控，根据告警信息，跟踪链路，定位错误日志信息
- 可回滚，代码/配置

一定要满足`可监控` 、`可灰度` 、 `可回滚`


## 运行中 【事中】
![故障监测](/images/sa_fail_identify.png)
### 故障发现(MTTI)
1.内部，配置告警，监测系统异常(基础资源、服务)
2.外部，舆情/客服/反馈平台获取。
### 故障认知(MTTK)
- tracing + Log + metric 快速定位

### 故障解决(MTTS)
![故障解决](/images/sa_fail_solve.png)
- 回滚，发现异常快速回滚线上变更，降低影响面
- 下线，下线故障节点
- 扩容，如果发现流量异常，及时扩容
- 容灾切换开关, 比如idc有异常，做流量切换
- 限流，针对服务、接口、appid做不同的限流配置。
- 熔断, 下游异常时，调整熔断配置，直接拒绝请求，避免资源浪费、堆积
- 降级，在发生异常时，可以通过打开开关，走降级逻辑，比如用户标签信息服务异常，可以先展示默认信息。
- 重启
- 代码bugfix

## 故障复盘【事后】
- 时间线 
- 根因分析
- 定责定级
- 优化逻辑
- 沉淀解决方案/工具，避免重蹈覆辙。

![稳定性各环节](/images/sa_process.png)


# 服务质量指标
主要包含SLA、SLI、SLO、这三个指标分别对应 `协议`、`指标`、`目标`。 
SLA一般是对外的，比如付费业务通常都会制定SLA协议，如果服务不满足协议要求，会有对应的赔偿方案，可参考[腾讯云SLA](https://cloud.tencent.com/document/product/301/103169#63ee1985-f56f-4629-afbf-cafde690ca64)
SLI和SLO关系好比是指标和值的关系，比如`push触达率:99.9%` 即 SLI: push触达率, SLO: 99.9%

## 如何制定质量指标？
在实际工程中，首先需要决策服务可用性采用"几个9"? 比如"99.9%", "99.99%"以及"99.999%"，可用性依次越来越高，至于选几个9不是拍脑袋定的。
### 第一步: 了解自身服务的特点
- 确认系统提供什么类型的服务?
- 确认不同故障的影响如何?
- 确认成本和可用性之间的关系？

#### 确认系统提供什么类型的服务?
1. 付费还是免费?
2. 是否对公司收入非常重要?
3. 竞争对手提供什么服务?
4. to B 还是 to C?

#### 确认不同故障的影响如何?
1. 确认计划外故障(断断续续故障(快速修复)，还是直接宕机(关闭服务))
2. 计划内故障

#### 确认成本和可用性之间的关系？
1. 提升一个9需要付出多少成本，两者之间的权衡


### 第二步: 确定服务质量指标
1. 可用性级别: 是否高可靠
2. 故障类型: 是否能容忍故障
3. 成本: 比如有的客户端要求延时，有的需要吞吐，需要根据不同需求场景，优化成本不一致


### 第三步:制定质量方案
1. 设置SLO、错误预算(裁决：功能迭代 vs  系统稳定)
2. 监控指标, 立体监控

<strong>如何选择SLI</strong>
这里需要说明下，`可用性时间`在某些场景下会产生歧义([SRE-计划外停机](https://sre.google/sre-book/embracing-risk/))，并不会作为SLI出现，相反成功率、请求延时、吞吐、连接数这些指标反而更常见。比如一个长连接网关系统，可能会更加关注连接数、延时、吞吐等
而评论系统可能更加关注吞吐、成功率等。所以SLI指标跟业务的关注点是分不开的。


## 参考资料
> [《Google SRE》](https://sre.google/sre-book/embracing-risk/)